
scenarios:
  Llama-3-8B-fp8-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-8B-Instruct
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-3-8B-fp8-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-8B-Instruct --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-3-70B-fp8-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-70B-Instruct
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --output_dir
    precision: fp8
  Llama-3-70B-fp8-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-70B-Instruct --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --output_dir
    precision: fp8
  Llama-2-7B-fp8-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-7b-chat-hf
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-7B-fp8-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-7b-chat-hf --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-13B-fp8-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-13b-chat-hf
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-13B-fp8-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-13b-chat-hf --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-70B-fp8-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-70b-chat-hf
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --output_dir
    precision: fp8
  Llama-2-70B-fp8-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-70b-chat-hf --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --output_dir
    precision: fp8

  Llama-3-8B-bf16-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-8B-Instruct
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --output_dir
    precision: bf16
  Llama-3-8B-bf16-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-8B-Instruct --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --output_dir
    precision: bf16
  Llama-3-70B-bf16-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-70B-Instruct
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --output_dir
    precision: bf16
  Llama-3-70B-bf16-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Meta-Llama-3-70B-Instruct --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --output_dir
    precision: bf16
  Llama-2-7B-bf16-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-7b-chat-hf
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --output_dir
    precision: bf16
  Llama-2-7B-bf16-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-7b-chat-hf --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --output_dir
    precision: bf16
  Llama-2-13B-bf16-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-13b-chat-hf
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --output_dir
    precision: bf16
  Llama-2-13B-bf16-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-13b-chat-hf --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --output_dir
    precision: bf16
  Llama-2-70B-bf16-FIXED-decode:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-70b-chat-hf
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --output_dir
    precision: bf16
  Llama-2-70B-bf16-FIXED-prefill:
    dataset: FIXED
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --tokenizer-path /mnt/weka/data/pytorch/llama2/Llama-2-70b-chat-hf --max-new-tokens 1
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --output_dir
    precision: bf16

  Llama-3-8B-fp8-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/fp8.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-3-8B-fp8-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/fp8.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-3-70B-fp8-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-3-70B-fp8-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-7B-fp8-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/fp8.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-7B-fp8-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/fp8.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-13B-fp8-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/fp8.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-13B-fp8-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/fp8.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-70B-fp8-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --fp8 --output_dir
    precision: fp8
  Llama-2-70B-fp8-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/fp8.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --fp8 --output_dir
    precision: fp8

  Llama-3-8B-bf16-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/bf16.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --output_dir
    precision: bf16
  Llama-3-8B-bf16-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama3-8b-99.9
    command: python main.py --user-conf configs/bf16.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 8b --scenario Server --output_dir
    precision: bf16
  Llama-3-70B-bf16-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --output_dir
    precision: bf16
  Llama-3-70B-bf16-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama3-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama3.
    init_Server: ./run_tgi_server.sh -m 70b3 --scenario Server --output_dir
    precision: bf16
  Llama-2-7B-bf16-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/bf16.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --output_dir
    precision: bf16
  Llama-2-7B-bf16-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/bf16.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 7b --scenario Server --output_dir
    precision: bf16
  Llama-2-13B-bf16-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/bf16.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --output_dir
    precision: bf16
  Llama-2-13B-bf16-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama2-13b-99.9
    command: python main.py --user-conf configs/bf16.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 13b --scenario Server --output_dir
    precision: bf16
  Llama-2-70B-bf16-orca-decode:
    dataset: orca
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --output_dir
    precision: bf16
  Llama-2-70B-bf16-orca-prefill:
    dataset: orca
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/bf16.conf --max-new-tokens 1 --dataset-path /root/llama2-70b-99.9/open_orca/open_orca_gpt4_tokenized_llama.
    init_Server: ./run_tgi_server.sh -m 70b --scenario Server --output_dir
    precision: bf16