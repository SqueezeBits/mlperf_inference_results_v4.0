benchmarks:
  llama2-70b-99.9:
    "rouge1": 43.83612
    "rouge2": 21.6890892
    "rougeL": 28.2219498
scenarios:
  llama-99.9-70b-fp8:
    dataset: orca
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py
    init_setup: ./setup_tgi.sh
    init_Offline: ./run_tgi_server.sh -m 70b --bs 1024 --scenario Offline --fp8 --output_dir
    init_Server: ./run_tgi_server.sh -m 70b --bs 768 --scenario Server --fp8 --output_dir
    precision: fp8
    batch_size_Offline: 1024
    batch_size_Server: 768
  llama-99.9-70b-bf16:
    dataset: orca
    code_dir: llama
    benchmark: llama2-70b-99.9
    command: python main.py --user-conf configs/bf16.conf
    init_setup: ./setup_tgi.sh
    init_Offline: ./run_tgi_server.sh -m 70b --bs 256 --scenario Offline --output_dir
    init_Server: ./run_tgi_server.sh -m 70b --bs 256 --scenario Server --output_dir
    precision: bf16
    batch_size_Offline: 256
    batch_size_Server: 256
  llama-99.9-7b-bf16:
    dataset: orca
    code_dir: llama
    benchmark: llama2-7b-99.9
    command: python main.py --user-conf configs/bf16.conf
    init_setup: ./setup_tgi.sh
    init_Offline: ./run_tgi_server.sh -m 7b --bs 16 --scenario Offline --output_dir
    init_Server: ./run_tgi_server.sh -m 7b --bs 16 --scenario Server --output_dir
    precision: bf16
    batch_size_Offline: 16
    batch_size_Server: 16